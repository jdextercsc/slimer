# 2.0. Logical Architecture Overview---## 2.1.	Openstack Technology OverviewThe Automation 2.0 OpenStack solution will be built using OpenSource technologies and solutions to deliver cloud IaaS computing.The Solution will consist of:- Controllers- Swift storage nodes- scaleio storage nodes- Hypervisor![Logical Mapping of Openstack Cluster](images/logical_mapping.png "Logical Mapping of Openstack Cluster")_Note: The diagram also displays the non-routed networks required for vxlan transport (applicable to tenant workload only) and the ScaleIO and Swift technologies. These technologies are defined in more detail further on in the document._  The control plane/API interconnectivity will be zoned as displayed above. Firewalls are presented on a server level for all ingress and egress.- There is internal managment and api network that all services communicate through.-	Client front end â€“ Controller and swift storage nodes will communicate on this network. HA Proxy servers will proxy connections to the controller and swift nodes for access to client facing services such as horizon dashboard and swift object store access.-	Compute - Nova compute and Neutron network nodes will be placed here. This will be where actual tenant workload is hosted. A vxlan transport network will enable client compute instance to utilise neutron network capabilities.---## 2.2. InfrastructureFrom an infrastructure perspective, an OpenStack Cloud is built on a foundation of virtual infrastructure, where components are split between a controller and cloud consumer resources to present client applications.In building an OpenStack Cloud the control components such as horizon dashboard will run in virtual machines.As a best practice, the resources allocated for management functions will be separate from pure user-requested workloads, the underlying controller nodes clusters will be split into two logical groups:-	A cluster of controller nodes running all core components and services needed to run the cloud-	Remaining available KVM compute nodes to provide vCPU and vRAM resources for consumption.Reasons for organizing and separating resources include:-	Ensuring that management components are separate from the resources they are managing.-	Minimizing overhead for cloud consumer resources. Resources allocated for cloud use have little overhead reserved.-	Dedicating resources for the cloud. Resources can be consistently and transparently managed and carved up and scaled horizontally.The underlying KVM Infrastructure will follow Linux KVM best practices. Design considerations specific to a OpenStack components will be addressed accordingly in this document, organized by the management nodes and cloud consumer resources.---## 2.3. Logical Build of Cluster Items### ControllerThe Controller nodes are responible for all of the API interactions with the services of OpenStack.  The controllers also provides databases, networking, as well as the OpenStack services.All openstack services are Active/Active unless otherwise specified.  The clustering of the services is handled via Pacemaker.*The use of pacemaker limits the number of controllers in the cluster to* **16** *this is a limitation of corosync.* Also note to keep quorum, there is a minimum of **3** controllers, and that and odd number of controllers should be maintained the ability to have quorum.  For quorum the cluster must have 51% of the voting cluster machines agree on the loss of a node(s).The Contoller nodes are serviced by 4 network connections, 2x 1Gbps connections and 2x 10Gbps fiber connections.  The 2 1Gbps connections are used for out of band mangement, and PXE and configuration.  The 2x 10Gbps connection are bonded together using 802.ad.  All of the vlans that are used for the openstack services are run on this bonded network interface.![Logical Map of Controller Nodes](images/controller_logical_submap.png)#### The core services provided by the controller group are:- mariadb, An sql database.  The sql database handles the database needs for the openstack services.  - Galera is used to provide redundancy to the database.  The mysql database is replicated between all controller nodes- mongodb database. A non-sql database used to store usage data from the ceilometer service.  Mongodb handles its own replication across a replication set that is defined in setup.- pacemaker / corosync, Pacemaker Cluster manager provides.  - detection and recovery of machine and application-level failures  - startup/shutdown ordering between applications  - preferences for other applications that must/must-not run on the same machine  - provably correct response to any failure or cluster state- haproxy loadbalancer, HAProxy is a free, open source high availability solution, providing load balancing and proxying for TCP and HTTP-based applications by spreading requests across multiple servers.- rabbitmq, a message bus providing Advanced Message Queuing Protocal (AMQP)- memcached, is a general-purpose distributed memory caching system#### Openstack ServicesThe services have been describe previous to this, please see section 1.2 for more info on each service.- Neutron (networking)  - Loadblancer as a Service (LBaaS)  - Firewall as a Service (FWaaS)- Glance (image service)- Cinder (block Storage)(**active/passive**)- Nova (non-compute)- Horizon (web ui)- Keystone (identity)- Heat (orchistration) (**active/passive**)- swift proxy (object storage)- ceilometer (metering)### Compute HypervisorThe compute hypervisor node supply KVM hypervisors to the enviroment by way of libvirt.  The hypervisor node has services for ceilometer as well as neutrons openvswitch.![Logical Map of Controller Nodes](images/compute_logical_submap.png)### Swift Storage nodeWhile swift proxy services are run on the controllers. The rest of the swift components are proivded by the storage nodes. These additional services are:- object server- account server- container serverThese storage nodes maintain redundancy via rysnc.  It is suggest to have a minimum of 3 swift nodes per the OpenStack admin guide best practices.### ScaleIO nodeScaleIO provides software-defined converged Server SAN.  ScaleIO consists of :- Metadata Manager (MDM)  - Configures and monitors the ScaleIO system.  - Keeps track of storage and data mapping  - Monitors capacity performance and Load balancing  - Makes data migration decisions- Gateway, Provides RESTful interface for interacting wtih the ScaleIO applicatoin- ScaleIO Data Server (SDS), Presents local storage to be used by ScaleIO.- ScaleIO Data Client (SDC),  Exposes the ScaleIO volumes to applications---[Table of Contents](toc.md) | [Back - Introduction](1.intro.md) | [Next - 2.0 Openstack Services Overview](2.0.osp_services.md)